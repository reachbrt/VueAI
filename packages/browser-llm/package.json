{
  "name": "@aivue/browser-llm",
  "version": "1.0.0",
  "description": "High-performance in-browser LLM inference for Vue.js using WebLLM and WebGPU",
  "main": "dist/index.js",
  "module": "dist/index.mjs",
  "types": "dist/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.mjs",
      "require": "./dist/index.js"
    },
    "./browser-llm.css": "./dist/browser-llm.css",
    "./dist/browser-llm.css": "./dist/browser-llm.css"
  },
  "files": [
    "dist",
    "README.md"
  ],
  "scripts": {
    "build": "npm run clean && vite build",
    "dev": "vite build --watch",
    "lint": "eslint \"src/**/*.{ts,vue}\"",
    "clean": "rm -rf dist",
    "prepublishOnly": "npm run build"
  },
  "keywords": [
    "vue",
    "ai",
    "llm",
    "browser",
    "webllm",
    "webgpu",
    "local-ai",
    "privacy",
    "offline",
    "machine-learning",
    "chatbot",
    "inference",
    "llama",
    "phi",
    "gemma",
    "mistral"
  ],
  "author": "reachbrt",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/reachbrt/vueai.git",
    "directory": "packages/browser-llm"
  },
  "homepage": "https://github.com/reachbrt/vueai#readme",
  "bugs": {
    "url": "https://github.com/reachbrt/vueai/issues"
  },
  "publishConfig": {
    "access": "public"
  },
  "peerDependencies": {
    "@aivue/core": "^1.3.5",
    "vue": "^2.6.0 || ^3.0.0"
  },
  "dependencies": {
    "@mlc-ai/web-llm": "^0.2.80"
  },
  "devDependencies": {
    "@types/node": "^20.16.0",
    "@vitejs/plugin-vue": "^5.0.0",
    "@vue/compiler-sfc": "^3.5.13",
    "eslint": "^8.56.0",
    "typescript": "^5.3.0",
    "vite": "^6.3.5",
    "vite-plugin-dts": "^4.5.3",
    "vue": "^3.5.0",
    "vue-tsc": "^2.2.10"
  }
}

